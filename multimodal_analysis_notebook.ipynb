{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "mgvsuixfrie27zklvb7y",
   "authorId": "3919642301233",
   "authorName": "JAMESE",
   "authorEmail": "james.cha-earley@snowflake.com",
   "sessionId": "5218f53b-5a1b-4630-95e2-3fda17455266",
   "lastEditTime": 1753986708911
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Multimodal Analysis with Snowflake Cortex AI\n",
    "\n",
    "This notebook demonstrates how to use Snowflake Cortex AI for comprehensive multimodal analysis, including image analysis with AI_COMPLETE and audio transcription with AI_TRANSCRIBE."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell2"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the required packages and set up our session:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell3",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import json\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# Setup session\n",
    "session = get_active_session()\n",
    "session.use_schema(\"MULTIMODAL_ANALYSIS.MEDIA\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "id": "144b3898-aef5-4389-b08a-3c9a2c55008f",
   "metadata": {
    "language": "sql",
    "name": "cell33"
   },
   "outputs": [],
   "source": "-- Create an image table that references the files in our stage\nCREATE OR REPLACE TABLE MULTIMODAL_ANALYSIS.MEDIA.IMAGE_TABLE AS\n  SELECT \n    RELATIVE_PATH AS image_path,\n    TO_FILE('@MULTIMODAL_ANALYSIS.MEDIA.IMAGES', RELATIVE_PATH) AS img_file\n  FROM DIRECTORY('@MULTIMODAL_ANALYSIS.MEDIA.IMAGES');\n\n-- Create an audio table that references the files in our stage\nCREATE OR REPLACE TABLE MULTIMODAL_ANALYSIS.MEDIA.AUDIO_TABLE AS\n  SELECT \n    RELATIVE_PATH AS audio_path,\n    TO_FILE('@MULTIMODAL_ANALYSIS.MEDIA.AUDIO', RELATIVE_PATH) AS audio_file\n  FROM DIRECTORY('@MULTIMODAL_ANALYSIS.MEDIA.AUDIO');\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "## View Available Media Files\n",
    "\n",
    "Let's see what images and audio files we have available:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# View available images\n",
    "image_df = session.sql(\"SELECT * FROM IMAGE_TABLE\").collect()\n",
    "print(f\"Found {len(image_df)} images in the stage\")\n",
    "\n",
    "# Display the first few images\n",
    "for i, row in enumerate(image_df[:3]):\n",
    "    print(f\"Image {i+1}: {row['IMAGE_PATH']}\")\n",
    "\n",
    "# View available audio files\n",
    "audio_df = session.sql(\"SELECT * FROM AUDIO_TABLE\").collect()\n",
    "print(f\"\\nFound {len(audio_df)} audio files in the stage\")\n",
    "\n",
    "# Display the first few audio files\n",
    "for i, row in enumerate(audio_df[:3]):\n",
    "    print(f\"Audio {i+1}: {row['AUDIO_PATH']}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell6"
   },
   "source": [
    "## Image Analysis with AI_COMPLETE\n",
    "\n",
    "Create a function to analyze images with AI_COMPLETE:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell7",
    "language": "python"
   },
   "outputs": [],
   "source": "# Function to analyze an image with chosen AI model\ndef analyze_image(image_path, prompt_template, model='claude-3-5-sonnet'):\n    # Escape single quotes in the prompt\n    escaped_prompt = prompt_template.replace(\"'\", \"''\")\n    \n    sql_query = f\"\"\"\n    SELECT \n      AI_COMPLETE(\n        '{model}', \n        '{escaped_prompt}',\n         TO_FILE('@IMAGES', '{image_path}')\n      )\n    \"\"\"\n    result = session.sql(sql_query).collect()\n    return result[0][0]",
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell8"
   },
   "source": [
    "## Basic Image Analysis\n",
    "\n",
    "Let's analyze our first image with a custom prompt:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell9",
    "language": "python"
   },
   "outputs": [],
   "source": "# Example analysis with custom prompt\nif len(image_df) > 0:\n    image_path = image_df[0]['IMAGE_PATH']  # Get first image\n    prompt = \"Describe what's happening in this image in the style of a detective investigating a scene. Be brief but detailed.\"\n\n    analysis = analyze_image(image_path, prompt)\n    print(f\"Analysis for {image_path}:\")\n    print(analysis)\nelse:\n    print(\"No images found in the table.\")",
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "## Object Detection\n",
    "\n",
    "Now let's identify objects in the image:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell11",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Object detection\n",
    "if len(image_df) > 0:\n",
    "    image_path = image_df[0]['IMAGE_PATH']\n",
    "    object_prompt = \"List all visible objects in this image and count how many there are of each type.\"\n",
    "    object_analysis = analyze_image(image_path, object_prompt)\n",
    "    print(\"Objects detected:\")\n",
    "    print(object_analysis)\n",
    "else:\n",
    "    print(\"No images found in the table.\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "## Text Extraction from Images\n",
    "\n",
    "Let's extract any visible text from the image:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell13",
    "language": "python"
   },
   "outputs": [],
   "source": "# Text extraction\nif len(image_df) > 0:\n    image_path = image_df[4]['IMAGE_PATH']\n    text_prompt = \"Extract and transcribe any visible text in this image. If no text is visible, respond with 'No text detected'.\"\n    text_analysis = analyze_image(image_path, text_prompt)\n    print(\"Text extracted:\")\n    print(text_analysis)\nelse:\n    print(\"No images found in the table.\")",
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell14"
   },
   "source": [
    "## Compare Different Models\n",
    "\n",
    "Let's compare Claude 3.5 Sonnet with Pixtral-large:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell15",
    "language": "python"
   },
   "outputs": [],
   "source": "# Try with different models\nif len(image_df) > 0:\n    image_path = image_df[0]['IMAGE_PATH']\n    prompt = \"Describe what you see in this image. Be detailed but concise.\"\n    \n    # Claude analysis\n    claude_analysis = analyze_image(image_path, prompt, model='claude-4-sonnet')\n    print(f\"Claude 4 Sonnet analysis for {image_path}:\")\n    print(claude_analysis)\n    \n    # Pixtral analysis\n    pixtral_analysis = analyze_image(image_path, prompt, model='pixtral-large')\n    print(f\"\\nPixtral-large analysis for {image_path}:\")\n    print(pixtral_analysis)\nelse:\n    print(\"No images found in the table.\")",
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell16"
   },
   "source": [
    "## Audio Transcription with AI_TRANSCRIBE\n",
    "\n",
    "Now let's work with audio files using AI_TRANSCRIBE:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell17",
    "language": "python"
   },
   "outputs": [],
   "source": "# Function to transcribe audio with different modes\ndef transcribe_audio(audio_path, mode='text'):\n    mode_options = {\n        'text': None,  # No second parameter for text mode\n        'word': \"{'timestamp_granularity': 'word'}\",\n        'speaker': \"{'timestamp_granularity': 'speaker'}\"\n    }\n    \n    if mode_options[mode] is None:\n        sql_query = f\"\"\"\n        SELECT AI_TRANSCRIBE(\n            TO_FILE('@AUDIO', '{audio_path}')\n        ) as transcription_result\n        \"\"\"\n    else:\n        sql_query = f\"\"\"\n        SELECT AI_TRANSCRIBE(\n            TO_FILE('@AUDIO', '{audio_path}'),\n            {mode_options[mode]}\n        ) as transcription_result\n        \"\"\"\n    \n    result = session.sql(sql_query).collect()\n    print(result)\n    return json.loads(result[0]['TRANSCRIPTION_RESULT'])",
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell18"
   },
   "source": [
    "## Basic Audio Transcription\n",
    "\n",
    "Let's transcribe our first audio file:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell19",
    "language": "python"
   },
   "outputs": [],
   "source": "# Basic text transcription\nif len(audio_df) > 0:\n    audio_path = audio_df[0]['AUDIO_PATH']\n    transcription = transcribe_audio(audio_path, mode='text')\n    \n    print(f\"Transcription for {audio_path}:\")\n    print(f\"Duration: {word_transcription['audio_duration']} seconds\")\n    print(f\"Text: {transcription['text']}\")\nelse:\n    print(\"No audio files found in the table.\")",
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell20"
   },
   "source": [
    "## Word-Level Timestamps\n",
    "\n",
    "Let's get word-level timestamps for precise navigation:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell21",
    "language": "python"
   },
   "outputs": [],
   "source": "# Word-level transcription\nif len(audio_df) > 0:\n    audio_path = audio_df[0]['AUDIO_PATH']\n    word_transcription = transcribe_audio(audio_path, mode='word')\n    \n    print(f\"Word-level transcription for {audio_path}:\")\n    print(f\"Duration: {word_transcription['audio_duration']} seconds\")\n    print(\"First 10 words with timestamps:\")\n    \n    for segment in word_transcription['segments'][:10]:\n        print(f\"  {segment['start']:.2f}s - {segment['end']:.2f}s: {segment['text']}\")\nelse:\n    print(\"No audio files found in the table.\")",
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell22"
   },
   "source": [
    "## Speaker Identification\n",
    "\n",
    "Let's identify different speakers in the audio:"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell23",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Speaker identification\n",
    "if len(audio_df) > 0:\n",
    "    audio_path = audio_df[0]['AUDIO_PATH']\n",
    "    speaker_transcription = transcribe_audio(audio_path, mode='speaker')\n",
    "    \n",
    "    print(f\"Speaker-segmented transcription for {audio_path}:\")\n",
    "    print(f\"Duration: {speaker_transcription['audio_duration']} seconds\")\n",
    "    \n",
    "    # Group by speaker\n",
    "    speakers = {}\n",
    "    for segment in speaker_transcription['segments']:\n",
    "        speaker = segment['speaker_label']\n",
    "        if speaker not in speakers:\n",
    "            speakers[speaker] = []\n",
    "        speakers[speaker].append(segment['text'])\n",
    "    \n",
    "    for speaker, texts in speakers.items():\n",
    "        print(f\"\\n{speaker}:\")\n",
    "        print(\" \".join(texts[:3]))  # Show first 3 segments\n",
    "else:\n",
    "    print(\"No audio files found in the table.\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell32",
    "collapsed": false
   },
   "source": "## Conclusion\n\nThis notebook demonstrates the full capabilities of Snowflake Cortex AI for multimodal analysis, combining powerful image analysis with AI_COMPLETE and comprehensive audio processing with AI_TRANSCRIBE.\n\nKey takeaways:\n- **AI_COMPLETE** provides sophisticated image analysis capabilities with multiple model options\n- **AI_TRANSCRIBE** offers flexible audio processing with text, word-level, and speaker identification modes\n\nExplore different combinations of prompts, models, and analysis types to find the best approach for your specific use case!",
   "id": "ce110000-1111-2222-3333-ffffff000031"
  }
 ]
}